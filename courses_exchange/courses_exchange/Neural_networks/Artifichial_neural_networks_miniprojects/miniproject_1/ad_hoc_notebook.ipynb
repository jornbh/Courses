{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "## Questions\n",
    "\n",
    "> ### Exercise 1: Data import and visualization (4 points)\n",
    "> \n",
    "> #### Description\n",
    "> \n",
    "> #### Loading the data\n",
    ">\n",
    "> The datasets we use in this project (MNIST, Fashion-MNIST) consists of grayscale images with 28x28 pixels. Keras comes with a convenient in-built [data importer](https://keras.io/datasets/) for common datasets.\n",
    ">\n",
    ">1. As a warm-up exercise, use this importer to (down-)load the MNIST and Fashion-MNIST dataset. Assign useful variables to test & train images and labels for both datasets respectively. (2 pts)\n",
    ">2. Use the corresponding plotting function defined above to plot some samples of the two datasets. What do the green digits at the bottom left of each image indicate? (1 sentence max.) (2 pts)\n",
    ">\n",
    ">The low resolution (and grayscale) of the images certainly misses some information that could be helpful for classifying the images. However, since the data has lower dimensionality due to the low resolution, the fitting procedures converge faster. This is an advantage in situations like here (or generally when prototyping), were we want to try many different things without having to wait too long for computations to finish.\n",
    "\n",
    "## Awnsers\n",
    "\n",
    "1. (No question asked, but a plot was given)\n",
    "2. ![Display of the samples](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_1/some_samples.png)\n",
    "3. The green numbers below each number is theri index in the table representing each different class ( This list is 1 -indexed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was made automatically, so I dumped ecerything in task 1\n",
    "\n",
    "# Everything needed to run task 4 and later (This keeps jupyter notebook from hogging all the memomy, and killing the PC)\n",
    "\n",
    "student1 = \"Jørn Bøni Hofstad\"\n",
    "student2 = \"Pleis H. Older\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# MY stuff \n",
    "GLOBAL_N_EPOCHS = 50 # Controll variable for debugging the program\n",
    "NELDER_MEAD_ITTERATIONS = 20\n",
    "DEBUG_BATCH_SIZE = 128\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "\n",
    "\n",
    "# Handed out functionaliry\n",
    "################################################################################\n",
    "# Ploting functionality\n",
    "def plot_some_samples(x, y = [], yhat = [], select_from = [], \n",
    "                      ncols = 6, nrows = 4, xdim = 28, ydim = 28,\n",
    "                      label_mapping = range(10)):\n",
    "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
    "    \n",
    "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
    "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from (if it is empty, select_from becomes range(N)).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y             -- corresponding labels to plot in green below each image.\n",
    "    yhat          -- corresponding predicted labels to plot in red below each image.\n",
    "    select_from   -- list of indices from which to select the images.\n",
    "    ncols, nrows  -- number of columns and rows to plot.\n",
    "    xdim, ydim    -- number of pixels of the images in x- and y-direction.\n",
    "    label_mapping -- map labels to digits.\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    if len(select_from) == 0:\n",
    "        select_from = range(x.shape[0])\n",
    "    indices = np.random.choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
    "    for i, ind in enumerate(indices):\n",
    "        thisax = ax[i//ncols,i%ncols]\n",
    "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
    "        thisax.set_axis_off()\n",
    "        if len(y) != 0:\n",
    "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
    "            thisax.text(0, 0, (label_mapping[j]+1)%10, color='green', \n",
    "                                                       verticalalignment='top',\n",
    "                                                       transform=thisax.transAxes)\n",
    "        if len(yhat) != 0:\n",
    "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
    "            thisax.text(1, 0, (label_mapping[k]+1)%10, color='red',\n",
    "                                             verticalalignment='top',\n",
    "                                             horizontalalignment='right',\n",
    "                                             transform=thisax.transAxes)\n",
    "    return fig\n",
    "def prepare_standardplot(title, xlabel):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_ylabel('categorical cross entropy')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_ylabel('accuracy [% correct]')\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    return fig, ax1, ax2\n",
    "def finalize_standardplot(fig, ax1, ax2):\n",
    "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
    "    if len(ax1labels) > 0:\n",
    "        ax1.legend(ax1handles, ax1labels)\n",
    "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
    "    if len(ax2labels) > 0:\n",
    "        ax2.legend(ax2handles, ax2labels)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "def plot_history(history, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history.history['loss'], label = \"training\")\n",
    "    ax1.plot(history.history['val_loss'], label = \"validation\")\n",
    "    ax2.plot(history.history['acc'], label = \"training\")\n",
    "    ax2.plot(history.history['val_acc'], label = \"validation\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig\n",
    "\n",
    "\n",
    "# This plotting routine might help you ...\n",
    "def comparison_plot(history_sgd, history_adam, label1, label2, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, \"epochs\")\n",
    "    ax1.plot(history_sgd.history['loss'], label=label1 + ' training')\n",
    "    ax1.plot(history_sgd.history['val_loss'], label=label1 + ' validation')\n",
    "    ax1.plot(history_adam.history['loss'], label=label2 + ' training')\n",
    "    ax1.plot(history_adam.history['val_loss'], label=label2 + ' validation')\n",
    "    ax2.plot(history_sgd.history['acc'], label=label1 + ' training')\n",
    "    ax2.plot(history_sgd.history['val_acc'], label=label1 + ' validation')\n",
    "    ax2.plot(history_adam.history['acc'], label=label2 + ' training')\n",
    "    ax2.plot(history_adam.history['val_acc'], label=label2 + ' validation')\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig\n",
    "\n",
    "\n",
    "# My owns stuff again\n",
    "################################################################################\n",
    "\n",
    "\n",
    "\n",
    "#Creating my own datasets \n",
    "########################################\n",
    "#Datasets\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(x_number_train_raw, y_number_train_raw), (x_number_test_raw, y_number_test_raw) = mnist.load_data()\n",
    "(x_fashion_train_raw, y_fashion_train_raw), (x_fashion_test_raw, y_fashion_test_raw) = fashion_mnist.load_data()\n",
    "\n",
    "x_number_train = x_number_train_raw.reshape(x_number_train_raw.shape[0], x_number_train_raw.shape[1]*x_number_train_raw.shape[2])/np.max(x_number_train_raw)\n",
    "x_number_test = x_number_test_raw.reshape(x_number_test_raw.shape[0], x_number_test_raw.shape[1]*x_number_test_raw.shape[2])/np.max(x_number_test_raw)\n",
    "\n",
    "x_fashion_train = x_fashion_train_raw.reshape(x_fashion_train_raw.shape[0], x_fashion_train_raw.shape[1]*x_fashion_train_raw.shape[2])/np.max(x_fashion_train_raw)\n",
    "x_fashion_test = x_fashion_test_raw.reshape(x_fashion_test_raw.shape[0], x_fashion_test_raw.shape[1]*x_fashion_test_raw.shape[2])/np.max(x_fashion_test_raw)\n",
    "\n",
    "y_number_train = keras.utils.to_categorical(y_number_train_raw)\n",
    "y_test = keras.utils.to_categorical(y_number_test_raw)\n",
    "\n",
    "y_fashion_train = keras.utils.to_categorical(y_fashion_train_raw)\n",
    "y_fashion_test = keras.utils.to_categorical(y_fashion_test_raw)\n",
    "\n",
    "x_number_train = x_number_train_raw.reshape(x_number_train_raw.shape[0], x_number_train_raw.shape[1]*x_number_train_raw.shape[2])/np.max(x_number_train_raw)\n",
    "\n",
    "x_number_test = x_number_test_raw.reshape(x_number_test_raw.shape[0], x_number_test_raw.shape[1]*x_number_test_raw.shape[2])/np.max(x_number_test_raw)\n",
    "\n",
    "x_fashion_train = x_fashion_train_raw.reshape(x_fashion_train_raw.shape[0], x_fashion_train_raw.shape[1]*x_fashion_train_raw.shape[2])/np.max(x_fashion_train_raw)\n",
    "x_fashion_test = x_fashion_test_raw.reshape(x_fashion_test_raw.shape[0], x_fashion_test_raw.shape[1]*x_fashion_test_raw.shape[2])/np.max(x_fashion_test_raw)\n",
    "\n",
    "# Libraries for making the neural networks\n",
    "########################################\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "# Function that will plot something /somewhere/, both in jupyter notebook, if run normally in the project\n",
    "########################################\n",
    "def reckless_savefig(fig, filename):\n",
    "    try: \n",
    "        fig.savefig(filename)\n",
    "        plt.close(fig)\n",
    "    except: \n",
    "        print(\"Unable to save plot with filename:\", filename )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    ">## Questions:\n",
    ">\n",
    ">### Exercise 2: No hidden layer (10 points)\n",
    ">\n",
    ">#### Description\n",
    ">\n",
    ">Define and fit a model without a hidden layer (since we will use multi-layer models later in this project, you can define a general constructor function for models with an arbitrary number of hidden layers already at this point). (1 pt for each step)\n",
    ">\n",
    ">1. Use the softmax activation for the output layer.\n",
    ">2. Use the categorical_crossentropy loss.\n",
    ">3. Add the accuracy metric to the metrics.\n",
    ">4. Choose stochastic gradient descent for the optimizer.\n",
    ">5. Choose a minibatch size of 128.\n",
    ">6. Fit for as many epochs as needed to see no further decrease in the validation loss.\n",
    ">7. Plot the output of the fitting procedure (a history object) using the function plot_history defined above.\n",
    ">8. Determine the indices of all test images that are misclassified by the fitted model and plot some of them using the function `plot_some_samples(x_test, y_test, yhat_test, error_indices)`. Explain the green and red digits at the bottom of each image.\n",
    ">9. Repeat the above steps for fitting the network to the Fashion-MNIST dataset.\n",
    ">\n",
    ">Hints:\n",
    ">\n",
    ">- Read the keras docs, in particular [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/).\n",
    ">- Have a look at the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_mlp](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py).\n",
    "\n",
    "## Awnsers\n",
    "\n",
    "1. (Just implementation)\n",
    "2. (Just implementation)\n",
    "3. (Just implementation)\n",
    "4. (Just implementation)\n",
    "5. (Just implementation)\n",
    "6. We end up using 50 epochs here, since it is consistent with later exercises, making debugging easier. But going above 50 might also be usefull, as it seems like the model still has not converged completely. Still, it seems like most of it has converged.\n",
    "\n",
    "7. The history of the two objects:\n",
    "    - ![Plot of learing ](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_2/Task_2_number.png)Task_2_fashion.png)\n",
    "8. The red number corresponds to the incorrect estimate, while the green one represents the ground truth.\n",
    "    - ![Pic](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_2/Task_2_Some_mislabeled_number_samples_after_training.png)\n",
    "\n",
    "9. The plots and code for fashion MNIST have already been added. (But the first picture, for numbers got the wrong name)\n",
    "    - ![Plot of learing ](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_2/Task_2_fashion.png)\n",
    "    - ![Pic](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_2/Task_2_Some_mislabeled_fashion_samples_after_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run task 1 if it is tun on a local computer \n",
    "# In a notebook, this should not be an issue\n",
    "try:     \n",
    "    from initialize_miniproject_1 import *\n",
    "except: \n",
    "    pass\n",
    "\n",
    "def make_model(depth, features=None, batch_size=None):\n",
    "    input_shape = features[0].shape\n",
    "\n",
    "    model = Sequential([Dense(32, input_shape=input_shape, batch_size=batch_size)])\n",
    "    for i in range(depth):\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(10))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"acc\"],\n",
    "        optimizer=optimizers.SGD()\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Get indexes of the samples the model can't learn correctly\n",
    "def get_mislabeled_indexes(model):\n",
    "    guesses = model.predict_classes(x_number_test).reshape((-1,))\n",
    "    ziped = ([(ind, guess) for ind, guess in enumerate(guesses)])\n",
    "\n",
    "    label_attempts = np.array([y_test[index] for index in ziped])\n",
    "    is_mislabeled = True != label_attempts\n",
    "\n",
    "    mislabeled_indexes = np.arange(is_mislabeled.shape[0])[is_mislabeled]\n",
    "\n",
    "    return mislabeled_indexes, guesses\n",
    "\n",
    "\n",
    "def task_2():  # Keep the momory from blowing up\n",
    "    fig = plot_some_samples(x_number_train, y_number_train)\n",
    "    reckless_savefig(fig, \"plots/task_2/Task_2_Some_arbitrary_numbers.png\")\n",
    "\n",
    "\n",
    "\n",
    "    # Testing the networks\n",
    "\n",
    "    # The cases we want to train on: (features, labels, model-name)\n",
    "    training_cases = [ \n",
    "        (x_number_train , y_number_train        , \"number\"  ),  \n",
    "        (x_fashion_train, y_fashion_train, \"fashion\" ),  \n",
    "    ]\n",
    "\n",
    "    for features, labels, name in training_cases: \n",
    "        batch_size = 128\n",
    "        model = make_model(2, features=features, batch_size=batch_size)\n",
    "        history = model.fit( \n",
    "                                    features, \n",
    "                                    labels,\n",
    "                                    epochs=GLOBAL_N_EPOCHS,\n",
    "                                    batch_size=batch_size,\n",
    "                                    validation_split=0.2\n",
    "                                )\n",
    "\n",
    "        # Plot history\n",
    "        history_path = \"plots/task_2/Task_2_{}.png\".format(name)\n",
    "        fig = plot_history(history, \"Task_2 {}\".format(name) )\n",
    "        reckless_savefig(fig, history_path)\n",
    "        mislabeled_indexes, guesses = get_mislabeled_indexes(model)\n",
    "\n",
    "        # Plot mislabeling\n",
    "        mislabeled_path = \"plots/task_2/Task_2_Some_mislabeled_{}_samples_after_training.png\".format(name)\n",
    "        fig = plot_some_samples(features, labels, guesses, mislabeled_indexes)\n",
    "        reckless_savefig(fig, mislabeled_path)\n",
    "\n",
    "\n",
    "task_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    ">## Qusestions\n",
    ">\n",
    ">### Exercise 3: One hidden layer, different optizimizers & overfitting (10 points)\n",
    ">\n",
    ">#### Description\n",
    ">\n",
    ">Train a network with one hidden layer and compare different optimizers.\n",
    ">\n",
    ">1. Use one hidden layer with 128 units and the 'relu' activation. Use the [summary method](https://keras.io/models/>about-keras-models/) to display your model in a compact way. (1 pt)\n",
    ">2. Fit the model for 50 epochs with different learning rates of stochastic gradient descent (SGD). (1pt)\n",
    ">3. Replace the stochastic gradient descent optimizer with the [Adam optimizer](https://keras.io/optimizers/#adam). (1pt)\n",
    ">4. Plot the learning curves of SGD with a reasonable learning rate (i.e. in the range [0.01,0.1]) together with the learning curves of Adam in the same figure. Take care of a reasonable labeling of the curves in the plot. (2pts)\n",
    ">5. Answer the questions below. (4pts)\n",
    ">6. Run the network (using the Adam optimizer) on the Fashion-MNIST dataset and plot the learning curves using the plot_history function defined above. (1pt)\n",
    "\n",
    "\n",
    "\n",
    "> ## Theoretical questions:\n",
    "\n",
    "# Task 3 awnsers\n",
    "\n",
    "> **Question**: What happens if the learning rate of SGD is A) very large B) very small? Please answer A) and B) with one full sentence each (double click this markdown cell to edit).\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "A) The range 0.01 to o.1, the stocastic gradiet descent converges more slowly when the learning rate is low. As the learning-rate increases, the accuracy of the training set increases more quickly. We also start seeing that the distance between the trainging set and the verification set becomes more noticeable as the learning rate increases.\n",
    "\n",
    "B) The Adams method has the advantage when the learning rate is very low, but it ends up performing more poorly when learnign rate is around 0.02. In addition, in these small examples, the asymptotic value of the adams method also seems to be noticeably worse tha\n",
    "\n",
    "> **Question**: At which epoch (approximately) does the Adam optimizer start to overfit (on MNIST)? Please answer with one full sentence.\n",
    "\n",
    "**Answer**: From the pictures, it can be seen that the adams method starts overfiting roughly around epoch 4\n",
    "\n",
    "> **Question**: Explain the qualitative difference between the loss curves and the accuracy curves with respect to signs of overfitting. Please answer with at most 3 full sentences.\n",
    "\n",
    "\n",
    "**Answer**: The accuracy function is based on slimple true or false for weterer or not the value that was guessed was correct or not. Loss, on the other hand is also affected by how confidently the model states something, giving a lower loss if the model is very confident on all the correct statements, but also taking a seere hit if any of the estimates it was so confident in proves to be wrong. So, if the loss decreases significantly, without any significant change in the accuracty, it is most likely a sign of that the model is overfiting. \n",
    "\n",
    "\n",
    "## Awnsers\n",
    "\n",
    " 1. (Just implementation)\n",
    " 2. (Just implementation)\n",
    " 3. (Just implementation)\n",
    " 4. All the plots: \n",
    "    1. <!-- TODO add plots -->\n",
    "\n",
    "\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_2/Task_2_fashion.png)\n",
    "\n",
    "\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_fashion,_learning_rate:_0.01,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_fashion,_learning_rate:_0.02,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_fashion,_learning_rate:_0.03,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_fashion,_learning_rate:_0.04,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_fashion,_learning_rate:_0.05,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_fashion,_learning_rate:_0.06,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_fashion,_learning_rate:_0.07,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_fashion,_learning_rate:_0.08,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_number,_learning_rate:_0.01,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_number,_learning_rate:_0.02,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_number,_learning_rate:_0.03,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_number,_learning_rate:_0.04,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_number,_learning_rate:_0.05,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_number,_learning_rate:_0.06,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_number,_learning_rate:_0.07,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_number,_learning_rate:_0.08,_epochs:_50.png)\n",
    "![Picture not found](https://github.com/jornbh/courses_exchange/tree/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_3/Task_3_learning_set:_number,_learning_rate:_0.09,_epochs:_50.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " 1. See above\n",
    " 2. Add plots from actually running on the fashion set\n",
    "<!-- ![Picture not found](./Plots_and_pictures/Exercise_3/Task 3 learning_set: number, learning_rate: 0.04, epochs: 50.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run task 1 if it is tun on a local computer \n",
    "# In a notebook, this should not be an issue\n",
    "try:     \n",
    "    from initialize_miniproject_1 import *\n",
    "except: \n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "#My code \n",
    "########################\n",
    "\n",
    "def make_model( features,labels, optimizer, learning_rate = None ): \n",
    "    # Input layer\n",
    "    model = Sequential( [Dense(32, input_shape=features[0].shape, batch_size = 128)] )\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Hidden layer\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(len(labels[0]))) \n",
    "    model.add( Activation(\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "                loss = \"categorical_crossentropy\", \n",
    "                metrics = [\"acc\"],\n",
    "                optimizer= optimizer(learning_rate)\n",
    "                )\n",
    "    return model\n",
    "\n",
    "# Variable parameters \n",
    "\n",
    "def compare_optimizers(\n",
    "    features = None, \n",
    "    labels = None, \n",
    "    learning_rate = None, \n",
    "    title = \"\"\n",
    "\n",
    "):\n",
    "    #1)\n",
    "    SGD_model  = make_model(features,labels, optimizers.SGD,  learning_rate=0.04)\n",
    "    Adam_model = make_model(features,labels, optimizers.Adam, learning_rate=0.04)\n",
    "    examples = [\n",
    "        (\"SGD\", SGD_model), \n",
    "        (\"Adam\", Adam_model)\n",
    "    ]\n",
    "    histories = {}\n",
    "    for name, model in examples:\n",
    "        history = model.fit(        features, \n",
    "                                    labels,\n",
    "                                    epochs = GLOBAL_N_EPOCHS,\n",
    "                                    batch_size = 128,\n",
    "                                    validation_split = 0.2,\n",
    "                                )    \n",
    "        histories[name]= history\n",
    "    comparison_title = \"Task 3 ( {} ) \".format(title)\n",
    "    comparison_path = \"plots/task_3/{}.png\".format(title)\n",
    "    fig = comparison_plot(histories[\"SGD\"], histories[\"Adam\"],\"SGD\", \"Adam\", title)\n",
    "    reckless_savefig(fig, comparison_path)\n",
    "    \n",
    "    \n",
    "def task_3():\n",
    "\n",
    "    # 3.1 to 3.3\n",
    "    compare_optimizers(features=x_number_train, labels=y_number_train, learning_rate=0.04, title=\"First comparison\")\n",
    "\n",
    "    learning_rate_list = np.arange(0.01, 0.1, 0.01)\n",
    "    learning_sets = [(x_fashion_train, y_fashion_train, \"Fashion MNIST\"), (x_number_train, y_number_train, \"NUMBER MNIST\")]\n",
    "\n",
    "\n",
    "\n",
    "    learning_sets = [\n",
    "        (x_fashion_train, y_fashion_train, \"fashion\"),\n",
    "        (x_number_train , y_number_train , \"number\"),\n",
    "    ]\n",
    "    # Implementation\n",
    "    for features, labels, name in learning_sets: \n",
    "        for learning_rate in learning_rate_list: \n",
    "            plot_title = \"Task 3 learning_set: {}, learning_rate: {}, epochs: {}\".format(name, round(learning_rate,2), GLOBAL_N_EPOCHS )\n",
    "            compare_optimizers( \n",
    "                                features=features,\n",
    "                                labels=labels,\n",
    "                                learning_rate=learning_rate,\n",
    "                                title=plot_title\n",
    "                            )\n",
    "task_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    ">## Questions\n",
    ">\n",
    ">### Exercise 4: Model performance as a function of number of hidden neurons (8 points)\n",
    ">\n",
    ">#### Description\n",
    ">\n",
    ">Investigate how the best validation loss and accuracy depends on the number of hidden neurons in a single layer.\n",
    ">\n",
    ">1. Fit a reasonable number of models (e.g. 5) with different hidden layer sizes (between 10 and 1000 hidden neurons) to the MNIST dataset. You may use the Adam optimizer and a meaningful number of epochs (overfitting!). (3 pts)\n",
    ">2. Plot the best validation loss and accuracy versus the number of hidden neurons. Is the observed trend in accordance with the [general approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)? If not, what might be practical reasons for the deviation? (2 sentences max.) (3 pts)\n",
    ">3. Repeat steps 1. & 2. for the Fashion-MNIST dataset. (2 pts)\n",
    ">\n",
    ">In this exercise we fit each model only for one initialization and random seed. In practice one would collect some statistics (e.g. 25-, 50-, 75-percentiles) for each layer size by fitting each model several times with different initializations and the random seeds. You may also want to do this here. It is a good exercise, but not mandatory as it takes quite a bit of computation time.\n",
    ">\n",
    ">#### Solution\n",
    "\n",
    "## Awnsers\n",
    "\n",
    "1. (Implementation)\n",
    "2. I messed in the original implementation, and the number of neurons in the hidden layer was writtn as depth \n",
    "   1. Best validation loss for each layer:\n",
    "   2. Best accuracy for each layer\n",
    "      * ![smallest fashion validation loss](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_4/smallest_losses_fashion.png)\n",
    "      * ![Smallest number validation loss](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_4/smallest_losses_number.png)\n",
    "      * It is not possible to see exactly how the validation loss is affected by the number of neurons in the single hiden layer. But it seems like adding a lot of hidden neuons causes the validation loss to go up. Most likely, this is because of the fact that the width increases the expressive power of the neural network, without increasing the bias towards learning the patterns that are common for all the pictures, but this is most likely wrong, since the plots form the 10 and 752 neuron case do not seem to entyrely agree with this\n",
    "         * ![Picture not found](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_4/Redundant_plots/Exercise_4_10_hidden_neurons.png)\n",
    "         * ![Picture not found](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_4/Redundant_plots/Exercise_4_752_hidden_neurons.png) \n",
    "      * It might be a bit more reasonable to simply state that for 50 epochs, the network does not imporove significantly by adding more hidden neurons, since most of the differences seem to come from the fact that the validation loss is so noisy. \n",
    "      * This seems to fit well with when we retry for the number MNIST \n",
    "      * ![Retry number MNIST](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_4/retry_smallest_losses_number.png)\n",
    "\n",
    "3. The fashion example is rather similair to the number example, and seem to display the same pattern as the number model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run task 1 if it is tun on a local computer \n",
    "# In a notebook, this should not be an issue\n",
    "try:     \n",
    "    from initialize_miniproject_1 import *\n",
    "except: \n",
    "    pass\n",
    "\n",
    "# Code to be tested\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "def make_model(features, labels, n_hidden_neurons):\n",
    "    model = Sequential(\n",
    "        [Dense(32, input_shape=features[0].shape, batch_size=128)])\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Hidden layer\n",
    "    model.add(Dense(n_hidden_neurons))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(len(labels[0])))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"acc\"],\n",
    "        optimizer=optimizers.Adam(0.01)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def task_4():\n",
    "\n",
    "        n_hidden_neurons_list = np.linspace(10, 1000, 5).astype(int)\n",
    "        smallest_losses = []\n",
    "        training_sets =[\n",
    "                        (x_number_train,  y_number_train , \"number\" ),\n",
    "                        (x_fashion_train, y_fashion_train, \"fashion\"),\n",
    "                ]   \n",
    "        for features, labels, name in training_sets:\n",
    "                for n_hidden_neurons in n_hidden_neurons_list:\n",
    "                        model = make_model(features, labels, n_hidden_neurons)\n",
    "                        model.summary()\n",
    "                        history = model.fit(\n",
    "                                                features,\n",
    "                                                labels,\n",
    "                                                epochs=GLOBAL_N_EPOCHS, \n",
    "                                                batch_size=128, \n",
    "                                                validation_split=0.2\n",
    "                                        )\n",
    "                        title= \"Exercise_4_({} MNIST,_{}_hidden_neurons)\".format(name, n_hidden_neurons)\n",
    "                        fig = plot_history(history, title )\n",
    "                        reckless_savefig(fig, \"plots/task_4/{}.png\".format(title))\n",
    "\n",
    "                        \n",
    "                        smallest_loss = np.min(history.history[\"loss\"])\n",
    "                        smallest_losses.append(smallest_loss)\n",
    "\n",
    "                print(smallest_losses)\n",
    "                shortest_len = min( len(smallest_losses), len(n_hidden_neurons_list) )\n",
    "                plt.plot(n_hidden_neurons_list[:shortest_len], smallest_losses[:shortest_len])\n",
    "                plt.title(\"Smallest validation loss for different number of hidden neurons \"+name)\n",
    "                plt.xlabel(\"n layers\")\n",
    "                plt.ylabel(\"loss\")\n",
    "                reckless_savefig(plt, \"./plots/task_4/MNIST_{}_compare_number_of_hidden_nodes.png\".format(name))\n",
    "\n",
    "\n",
    "task_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    ">## Questions\n",
    ">### Exercise 5: Going deeper: tricks and regularization (8 points)\n",
    ">\n",
    ">#### Description\n",
    ">\n",
    ">Adding hidden layers to a deep network does not necessarily lead to a straight-forward improvement of performance. Overfitting can be counteracted with regularization and dropout. Batch normalization is supposed to mainly speed up convergence. Since the MNIST dataset is almost perfectly solved already by a one-hidden-layer network we use the Fashion-MNIST dataset in this exercise.\n",
    ">\n",
    ">1. Add one or two hidden layers with 50 hidden neurons (each) and train the network for a sufficiently long time (at least 100 epochs). Since deep models are very expressive you will most probably encounter overfitting. Try to improve the best validation scores of the model (even if it is only a minor improvement) by experimenting with batch_normalization layers, dropout layers and l1- and l2-regularization on weights (kernels) and biases. (4 pts)\n",
    ">2. After you have found good settings, plot the learning curves for both models, naive (=no tricks/regularization) and tuned (=tricks + regularized), preferably together in a comparison plot. Discuss your results; refer to the model performance with only 1 hidden layer. (2 sentences max.) (2pts)\n",
    ">3. Fit your best performing (probably regularized deep) model also to MNIST for having a reference for the next exercise. Plot the resulting learning curves. (2 pts)\n",
    ">\n",
    ">#### Solution\n",
    "\n",
    "## Awnsers \n",
    "\n",
    "1. All the results can be seen below, and all of them seem to be acting very strangely.ch The model seems to have a sudden drop in accuracy when using a learning rate of 0.7 if batch normalization or l2 normalization is used. I was not able to pinpoint why this hapened, since I expected numerical instability to cause some kind of oscilation, which would be ovserved as a gradual drop. This problem was solved by changing the learning rate to 0.1, but there might exist more reasonable ways to handle this, like an adaptive step-size with potential backsteping, or potentially changing the loss function. But this causes the model to not fully converge in the 100 epochs, but from the observations that can be seen: \n",
    "\n",
    "None of the changes seem to have any noticeable effect.\n",
    "   1.  \n",
    "   2. Adding l1 regularisation \n",
    "   3. Adding l2 regularisation \n",
    "   4. adding bias\n",
    "\n",
    "\n",
    "1. Addding a bias term\n",
    "   * Adding a bias term makes the network able to represetn more, as it can take in a constant 1 as one of its inputs to each neuron. This can make the network able to learn more, but in this case, there was no visible effect. \n",
    "   * ![Picture not found](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_5/Task_5_1_case%3A_%7B%7Chas_bias%7C%3A_True%7D_epochs%3A_100.png)\n",
    "2. Adding l1 regularisation of 0.001 \n",
    "   * Adding a l1 regularisation term \n",
    "   * Usually, if a network is overfited, it puts a very large emphasis on a specific path, so certain wiaghts can become very large or very small. We can add regularisation terms to try to combat this, since the general intuition about pictures is rather that arrge areas are usually more important than single pixels. In this case, however, the difference between modified and unmodified was not vert noticeable. \n",
    "   * ![Picture not found](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_5/Task_5_1_case%3A_%7B%7Cl1_regularisation%7C%3A_0.001%7D_epochs%3A_100.png) \n",
    "3. Adding a dropout of 30%\n",
    "   * This is usually done in order to keep the network from relying too much on a single path when identifying something. In practice, this aparantly works very well. In this case, the difference was not very noticeable, since the network did not start overfiting yet. \n",
    "   * ![Picture not found](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_5/Task_5_1_case%3A_%7B%7Cdropout_rate%7C%3A_0.3%7D_epochs%3A_100.png          )\n",
    "4. Adding l2-regularisation\n",
    "   * Same idea as with l1-regularisation, except for that this one will punish single neurons who have extreme values harder than multiple neurons simply having high values. Onece again, the difference is not very noticeable. \n",
    "   * ![Picture not found](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_5/Task_5_1_case%3A_%7B%7Cl2_reguarisation%7C%3A_0.001%7D_epochs%3A_100.png)\n",
    "5. Adding batch-normalisation. \n",
    "   * This is mosyly a trick to reduce the effects of the vanishing gradient problem, so it stands to no suprise that the difference is hardly noticeable here as well\n",
    "* ![Picture not found](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_5/Task_5_1_case%3A_%7B%7Cis_batch_normalized%7C%3A_True%7D_epochs%3A_101.png)\n",
    "\n",
    "6. Ploting the best case against the unchanged example \n",
    "   * In this case, the l1-regularized network was the one who performed the best. But as we can see form the plot, there is almost no difference between the two. Potentially, if more running time were to be given, we might be able to see some difference as the networks would start to overfit, git in this case, we will continue to use the unmodifyed network for Exercise 6, due to simplicitty. \n",
    "   * ![](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_5/Task_5_2_no_movification_vs_%7B'l1_regularisation'%3A%200.001%7D_(Best_modification).png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run task 1 if it is tun on a local computer \n",
    "# In a notebook, this should not be an issue\n",
    "try:     \n",
    "    from initialize_miniproject_1 import *\n",
    "except: \n",
    "    print(\"Unable to import initialize_miniproject_1\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    my_n_epochs = GLOBAL_N_EPOCHS*2\n",
    "    test_case_list = [\n",
    "                {\"is_batch_normalized\"  :     True      },\n",
    "                {\"l1_regularisation\"    :     0.001     },\n",
    "                {\"l2_reguarisation\"     :     0.001     },\n",
    "                {\"has_bias\"             :     True      },\n",
    "                {\"dropout_rate\"         :     0.3       },\n",
    "    ]\n",
    "\n",
    "    # Part 1) \n",
    "    best_test_case   = None\n",
    "    best_history     = None \n",
    "    highest_accuracy = 0 \n",
    "    learning_rate = 0.0001\n",
    "    for test_case in test_case_list:\n",
    "        model = make_model(x_fashion_train, y_fashion_train, **test_case, learning_rate=learning_rate)\n",
    "        model.summary()\n",
    "        history = model.fit(    x_fashion_train,\n",
    "                                 y_fashion_train,\n",
    "                                 epochs = my_n_epochs,\n",
    "                                 batch_size = DEBUG_BATCH_SIZE,\n",
    "                                 validation_split = 0.2, \n",
    "                                 shuffle=True\n",
    "                            )\n",
    "        \n",
    "        # Plot the result\n",
    "        title = \"Task_5_1_case:_{}_epochs:_{}\".format(test_case, my_n_epochs)\n",
    "        title = title.replace(\" \", \"_\").replace(\"'\", \"|\")\n",
    "        path =\"./plots/task_5/\"+title+\".png\"\n",
    "        fig = plot_history(history, title)\n",
    "        reckless_savefig(fig, path)\n",
    "\n",
    "        if max( history.history[\"acc\"] ) > highest_accuracy:\n",
    "            highest_accuracy = max( history.history[\"acc\"] )\n",
    "            best_history = history\n",
    "            best_test_case = test_case\n",
    "\n",
    "    # Part 2) \n",
    "    untouched_model = make_model(x_fashion_train, y_fashion_train, learning_rate=learning_rate)\n",
    "    untouched_history = untouched_model.fit(x_fashion_train, y_fashion_train, epochs = my_n_epochs, batch_size = DEBUG_BATCH_SIZE, validation_split = 0.2)\n",
    "    fig = comparison_plot( best_history, untouched_history, str(best_test_case), \"Untouched model\", \"Task 5 best modification vs no modification \")\n",
    "    path = \"./plots/task_5/Task_5_2_no_movification_vs_{}_(Best_modification).png\".format(best_test_case)\n",
    "    reckless_savefig(fig, path)\n",
    "    # Part 3) \n",
    "    best_trick_number_model = make_model(x_number_train,y_number_train, **best_test_case)\n",
    "    number_history = history = model.fit(x_number_train, y_number_train, epochs = my_n_epochs, batch_size = DEBUG_BATCH_SIZE, validation_split = 0.2)\n",
    "    title = \"Task_5_3_best_case_{}_on_numbers_(epochs:_{})\".format(best_test_case, my_n_epochs)\n",
    "    fig = plot_history(number_history, title)\n",
    "    reckless_savefig(fig, \"./plots/task_5/\"+title+\".png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Implementation\n",
    "##################################################\n",
    "\n",
    "\n",
    "def make_hidden_layer_list(   \n",
    "                        is_batch_normalized=False,\n",
    "                        l1_regularisation=0,\n",
    "                        l2_regularisation=0, \n",
    "                        has_bias=False, \n",
    "                        dropout_rate=0  \n",
    "                    ):\n",
    "    \n",
    "    # Condition for if a layer should be included, and the layer\n",
    "    conditional_layers= [\n",
    "        (               True        ,   Dense(50, activation=\"linear\", use_bias=has_bias)       ),\n",
    "        (is_batch_normalized == True,   keras.layers.normalization.BatchNormalization()         ), \n",
    "        (l1_regularisation    >  0  ,   keras.layers.regularizers.l1(l1_regularisation)         ),\n",
    "        (l2_regularisation    >  0  ,   keras.layers.regularizers.l2(l2_regularisation)         ),\n",
    "        (dropout_rate         >  0  ,   keras.layers.Dropout(dropout_rate)                      ),\n",
    "    ]\n",
    "\n",
    "    # Filter the list\n",
    "    hidden_layer_list = []\n",
    "    for condition, layer in conditional_layers:\n",
    "        if condition == True: \n",
    "            hidden_layer_list.append(layer)\n",
    "    \n",
    "    return hidden_layer_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_model(     \n",
    "                    features, \n",
    "                    labels, \n",
    "                    learning_rate =0.01 ,\n",
    "                    is_batch_normalized=False,\n",
    "                    l1_regularisation=0,\n",
    "                    l2_reguarisation=0,\n",
    "                    has_bias=False,\n",
    "                    dropout_rate =0,\n",
    "                ):\n",
    "\n",
    "    \n",
    "    Network = [\n",
    "                        # Input layer\n",
    "                            Dense(32, input_shape=features[0].shape, batch_size = DEBUG_BATCH_SIZE)   ,\n",
    "                            Activation(\"linear\")                                                      ,\n",
    "    \n",
    "                        #Hidden layers\n",
    "                            *make_hidden_layer_list(), #! MAKE SURE YOU UNDERSTAND THE * on lists and tuples\n",
    "                            *make_hidden_layer_list(), # (It is as if you were to write all the elements in cleartext, separated by commas)\n",
    "                        \n",
    "                        # Output layer\n",
    "                            Dense( len(labels[0]) )                                                   ,\n",
    "                            Activation(\"softmax\")                                                     ,\n",
    "            \n",
    "            ]\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    for i in Network:\n",
    "        print(i)\n",
    "        model.add(i)\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "                loss = \"categorical_crossentropy\", \n",
    "                metrics = [\"acc\"],\n",
    "                optimizer= optimizers.Adam(learning_rate),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "\n",
    ">## Questions\n",
    ">### Exercise 6: Convolutional neural networks (CNNs) (10 points)\n",
    ">\n",
    ">#### Description\n",
    ">\n",
    ">Convolutional neural networks have an inductive bias that is well adapted to image classification.\n",
    ">\n",
    ">1. Design a convolutional neural network, play with different architectures and parameters. Hint: You may get valuable inspiration from the keras [examples](https://github.com/keras-team/keras/tree/master/examples). (4 pts)\n",
    ">2. Plot the learning curves of the convolutional neural network for MNIST and Fashion-MNIST. (4 pts)\n",
    ">3. How does the CNN performance compare to the so far best performing (deep) neural network model for the two data sets? (2 sentences max.) (2 pts)\n",
    ">\n",
    ">#### Solution\n",
    "\n",
    "\n",
    "## Awnsers\n",
    "\n",
    "1. (Implementation) \n",
    "   1. When I implemented this, I used data from the previous exercise that seemed to indicate that none of he made any change on \n",
    "   2. I only tried two cases for the convolutional network. One that had only a single hidden layer, but with three convolutional layers, and one with a single, larger convolutional layer, but with two fully connected layers. In both cases, I ended up using max-pooling, even though we were told not to. It seemed easier, and makes intuitively more sense when looking for features, (Alhough, i guess when going for intuition, and ignoring the math, taking the max based on aboslute values makes even more sense when we are looking for certain properties.)\n",
    "2. I somehow managed to crash the program before it was able to train the network with fewer convolutions. But I still have the plots for the one with many convolutions: \n",
    "   * ![Many convolutions on Fashion MNIST](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_6/Fashion_MNISTmany_convolutions_network.png)\n",
    "   * ![Many convolutions on Number MNIST](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_6/MNIST_many_convolutions_network.png)\n",
    "3. The Convolutional neural networks are more or less able to blow the old networks completely out of the water. We do see some difference in the accuracy between the training and the test-examples when it comes to fashion-MNIST, but the old networks were hardly able to get an accuracy above 85% on the training set. In the case of teh normal number MNIST, it has near perfect accuracy in both the training cases, as well as in the validation case. \n",
    "   * ![Nonconvolving network](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_6/task_6_naive_nonconvolving_depth=2_epochs_100.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run task 1 if it is tun on a local computer \n",
    "# In a notebook, this should not be an issue\n",
    "try:     \n",
    "    from initialize_miniproject_1 import *\n",
    "except: \n",
    "    pass\n",
    "\n",
    "    \n",
    "def log(*args, file= None, **kwargs): \n",
    "    try:\n",
    "        if log_file == None: \n",
    "            log_file = open(\"logs/task_6/main_log\", \"a\") \n",
    "            print(*args, **kwargs, file=log_file)\n",
    "            log_file.close()\n",
    "        else: \n",
    "            print(*args, **kwargs, file=log_file)\n",
    "    except: \n",
    "        print(\"Unable to write to log\")\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "log(\"#\"*26, \"Starting new run of task 6\", sep=\"\\n\")\n",
    "\n",
    "# Convolutional neural networks\n",
    "\n",
    "def make_one_convolution_model(training_set, label_set): \n",
    "    # One convolution, two layers\n",
    "    learning_rate = 0.004\n",
    "    # n_labels = label_set.shape[1]\n",
    "    num_classes = 10\n",
    "\n",
    "    # Single later, but several convolutional layers\n",
    "    output_model = Sequential()\n",
    "    output_model.add(Conv2D(32*2, kernel_size=(4, 4),activation='linear',input_shape=(28,28,1),padding='same'))\n",
    "    output_model.add(Activation(\"relu\"))\n",
    "    output_model.add(MaxPooling2D((4, 4),padding='same')) # I use max-polling because I decide to be stupid and not listen to others\n",
    "\n",
    "    output_model.add(Flatten()) # We need a flattening layer (?) \n",
    "    output_model.add(Dense(128, activation='linear'))\n",
    "    output_model.add(Activation(\"relu\"))  \n",
    "    output_model.add(Dense(128, activation='linear'))\n",
    "    output_model.add(Activation(\"relu\"))                   \n",
    "    output_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    output_model.compile( \n",
    "                    loss= \"categorical_crossentropy\",\n",
    "                    metrics=[\"acc\"],\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate)\n",
    "    )\n",
    "    output_model.summary()\n",
    "    return output_model\n",
    "\n",
    "\n",
    "def make_many_convolutions_model(training_set, label_set, learning_rate = 0.004): \n",
    "    # n_labels = label_set.shape[1]\n",
    "    num_classes = 10\n",
    "\n",
    "    # Single later, but several convolutional layers\n",
    "    output_model = Sequential()\n",
    "    output_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\n",
    "    output_model.add(Activation(\"sigmoid\"))\n",
    "    output_model.add(MaxPooling2D((2, 2),padding='same')) # I use max-polling because I decide to be stupid and not listen to others\n",
    "    output_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "    output_model.add(Activation(\"sigmoid\"))\n",
    "    output_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    output_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "    output_model.add(Activation(\"sigmoid\"))                  \n",
    "    output_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    output_model.add(Flatten())\n",
    "    output_model.add(Dense(128, activation='linear'))\n",
    "    output_model.add(Activation(\"sigmoid\"))                  \n",
    "    output_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    output_model.compile( \n",
    "                    loss= \"categorical_crossentropy\",\n",
    "                    metrics=[\"acc\"],\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate)\n",
    "    )\n",
    "    output_model.summary()\n",
    "    return output_model\n",
    "\n",
    "def task_6_1(\n",
    "    learning_x = x_number_train_raw[:,:,:,None],\n",
    "    learning_y = y_number_train, \n",
    "    training_set_name = \"MNIST\"\n",
    "): \n",
    "\n",
    "    my_n_epochs = GLOBAL_N_EPOCHS*2\n",
    "\n",
    "    # Test the different models\n",
    "    many_convolutions_model = make_many_convolutions_model(learning_x, learning_y)\n",
    "    one_convolution_model = make_one_convolution_model(learning_x, learning_y)\n",
    "    \n",
    "    log(\"Start training the many convolutions net\")\n",
    "    many_convolutions_history = many_convolutions_model.fit(learning_x, learning_y, epochs = my_n_epochs, batch_size=DEBUG_BATCH_SIZE, validation_split=0.2)\n",
    "    fig = plot_history(many_convolutions_history, \"Task 6 \"+training_set_name+\" (Many convolutions) \")\n",
    "    reckless_savefig(fig, \"plots/task_6/\"+training_set_name+\"many_convolutions_network.png\")\n",
    "    # fig.savefig(\"plots/task_6/many_convolutions_network.png\")\n",
    "    \n",
    "    plt.close(fig)\n",
    "    log(\"Start training the one convolution, two layer net\")\n",
    "\n",
    "    one_convolution_history = one_convolution_model.fit( learning_x, learning_y, epochs = my_n_epochs, batch_size=DEBUG_BATCH_SIZE, validation_split=0.2)\n",
    "    fig = plot_history(one_convolution_history, \"Task 6 \"+training_set_name+\" (One convolution, 2 hidden layers)\")\n",
    "    reckless_savefig(fig, \"plots/task_6/\"+training_set_name+\"__one_convolution_network.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    task_6_1(\n",
    "        learning_x          =x_number_train_raw[:,:,:,None],\n",
    "        learning_y          =y_number_train,\n",
    "        training_set_name   =\"MNIST_\"\n",
    "    )\n",
    "\n",
    "    task_6_1(\n",
    "        learning_x         = x_fashion_train_raw[:,:,:,None],\n",
    "        learning_y         = y_fashion_train,\n",
    "        training_set_name  =\"Fashion_MNIST\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7\n",
    "\n",
    ">## Questions\n",
    ">### Exercise 7: Sigmoidal activation function and batch-normalization (6 >points)\n",
    ">\n",
    ">#### Description:\n",
    ">\n",
    ">In the original publication of batch normalization [Ioffe and Szegedy, 2014](https://arxiv.org/pdf/1502.03167.pdf), the authors mention a particularly beneficial effect of their method on networks with sigmoidal activation functions. This is because such networks usually suffer from saturating activations/vanishing gradients. Here we want to reproduce this behaviour (Chose either MNIST or Fashion-MNIST for this exercise).\n",
    ">\n",
    ">1. Implement the same convolutional network as in the previous exercise, but using the sigmoid activation function instead of the standard choice ReLU. Train the network for a reasonable amount of time. What do you observe? (1 sentence max.) (3 pts)\n",
    ">2. Add batch-normalization layers to all convolutional and fully-connected layers (i.e. before each layer with learnable parameters). How does the performance change? Can the network reach the ReLU-CNN performance of the previous exercise? (1 sentence max.) (3 pts)\n",
    ">3. **BONUS (optional, not graded**): Investigate our initial guess that saturating activity/vanishing gradients might be the cause of this behaviour. For that, create histograms of the hidden activitions for different hidden layers for the sigmoid-CNN and the sigmoid-CNN with batch-normalization (counting over both, samples and neurons per layer). You may only chose layers with learnable parameters. What do you observe?\n",
    ">Hint: You can use the [keract](https://github.com/philipperemy/keract) package to access neural activation values for all layers of your network model.\n",
    ">\n",
    ">\n",
    ">\n",
    ">#### Solution:\n",
    "\n",
    "## Awnsers\n",
    "\n",
    "1. We see that for the training example it is mostly similair to the provious cases, but when it comes to the validation test, it suffers greatly from some kind of noise or uncertainty. \n",
    "   * ![Picture not found](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_7/Fashion_MNISTmany_convolutions_network_unnormalized.png)\n",
    "   * The result of the sigmoidal function is that the accuracy of the validation seems to have remained the same. In this case, the network is not very deep, so we do not suffer from the vanishing gradients problem, so we do not take a severe hit in the number of itterations needed for convergence eiter. (Both the case form Exercise 5 and 6 seems to have more or less converged after 50 epochs)\n",
    "   * In the case of the batch-normalized example. It can be seen that it reaches a similair level of accuracy at the validation set. \n",
    "   * ![Picture not found](https://raw.githubusercontent.com/jornbh/courses_exchange/master/Neural_networks/Ovinger/miniproject_1/Awnsers/Plots_and_pictures/Exercise_7/Fashion_MNISTmany_convolutions_network_batch_normalized.png)\n",
    "   * Since the network is not particulary deep, the result of the batch normalisation does not end up leadning to a large improvement in convergence rate, since both validation sets seems to convergge somewhere around epoch 20, and the training sets seem to have almost completely converged at epoch 75. Still, if this had been a deep network, the difference would have been more noticeable, and the batch-normalized network should have conberged faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run task 1 if it is tun on a local computer \n",
    "# In a notebook, this should not be an issue\n",
    "try:     \n",
    "    from initialize_miniproject_1 import *\n",
    "except: \n",
    "    pass\n",
    "\n",
    "def log(*args, file= None, **kwargs): \n",
    "    try:\n",
    "        if log_file == None: \n",
    "            log_file = open(\"logs/task_7/main_log\", \"a\") \n",
    "            print(*args, **kwargs, file=log_file)\n",
    "            log_file.close()\n",
    "        else: \n",
    "            print(*args, **kwargs, file=log_file)\n",
    "    except: \n",
    "        print(\"Unable to write to log\")\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "log(\"#\"*26, \"Starting new run of task 6\", sep=\"\\n\")\n",
    "\n",
    "# Convolutional neural networks\n",
    "\n",
    "def make_many_convolutions_model_unnormalized(training_set, label_set, learning_rate = 0.004): \n",
    "    # n_labels = label_set.shape[1]\n",
    "    num_classes = 10\n",
    "\n",
    "    # Single later, but several convolutional layers\n",
    "    output_model = Sequential()\n",
    "    output_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\n",
    "    output_model.add(Activation(\"sigmoid\"))\n",
    "    output_model.add(MaxPooling2D((2, 2),padding='same')) # I use max-polling because I decide to be stupid and not listen to others\n",
    "    output_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "    output_model.add(Activation(\"sigmoid\"))\n",
    "    output_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    output_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "    output_model.add(Activation(\"sigmoid\"))                  \n",
    "    output_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    output_model.add(Flatten())\n",
    "    output_model.add(Dense(128, activation='linear'))\n",
    "    output_model.add(Activation(\"sigmoid\"))                  \n",
    "    output_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    output_model.compile( \n",
    "                    loss= \"categorical_crossentropy\",\n",
    "                    metrics=[\"acc\"],\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate)\n",
    "    )\n",
    "    output_model.summary()\n",
    "    return output_model\n",
    "\n",
    "def make_many_convolutions_model_batch_normalized(training_set, label_set, learning_rate = 0.004): \n",
    "    # n_labels = label_set.shape[1]\n",
    "    num_classes = 10\n",
    "\n",
    "    # Single later, but several convolutional layers\n",
    "    output_model = Sequential()\n",
    "    output_model.add(keras.layers.BatchNormalization()                                                                                           ) \n",
    "    output_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same')                                     ) \n",
    "    output_model.add(Activation(\"sigmoid\")                                                                                                       ) \n",
    "    output_model.add(MaxPooling2D((2, 2),padding='same')) # I use max-polling because I decide to be stupid and not listen to others \n",
    "    output_model.add(keras.layers.BatchNormalization()                                                                                           ) \n",
    "    output_model.add(Conv2D(64, (3, 3), activation='linear',padding='same')                                                                      ) \n",
    "    output_model.add(Activation(\"sigmoid\")                                                                                                       ) \n",
    "    output_model.add(MaxPooling2D(pool_size=(2, 2),padding='same')                                                                               ) \n",
    "    output_model.add(keras.layers.BatchNormalization()                                                                                           ) \n",
    "    output_model.add(Conv2D(128, (3, 3), activation='linear',padding='same')                                                                     ) \n",
    "    output_model.add(Activation(\"sigmoid\")                                                                                                       )                   \n",
    "    output_model.add(MaxPooling2D(pool_size=(2, 2),padding='same')                                                                               ) \n",
    "    output_model.add(Flatten()                                                                                                                   ) \n",
    "    output_model.add(keras.layers.BatchNormalization()                                                                                           ) \n",
    "    output_model.add(Dense(128, activation='linear')                                                                                             ) \n",
    "    output_model.add(Activation(\"sigmoid\")                                                                                                       )                   \n",
    "    output_model.add(Dense(num_classes, activation='softmax')                                                                                    ) \n",
    "\n",
    "    output_model.compile( \n",
    "                    loss= \"categorical_crossentropy\",\n",
    "                    metrics=[\"acc\"],\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate)\n",
    "    )\n",
    "    # output_model.summary()\n",
    "    return output_model\n",
    "\n",
    "\n",
    "def task_7_1(\n",
    "    learning_x = x_number_train_raw[:,:,:,None],\n",
    "    learning_y = y_number_train, \n",
    "    training_set_name = \"MNIST\"\n",
    "): \n",
    "\n",
    "    my_n_epochs = GLOBAL_N_EPOCHS*2\n",
    "\n",
    "    # Test the different models\n",
    "    many_convolutions_model_unnormalized = make_many_convolutions_model_unnormalized(learning_x, learning_y)\n",
    "   \n",
    "    log(\"Start training the many convolutions net\")\n",
    "    many_convolutions_history_unnormalized = many_convolutions_model_unnormalized.fit(learning_x, learning_y, epochs = my_n_epochs, batch_size=DEBUG_BATCH_SIZE, validation_split=0.2)\n",
    "    fig = plot_history(many_convolutions_history_unnormalized, \"Task 7 \"+training_set_name+\" (Many convolutions (Unnormalised)) \")\n",
    "    reckless_savefig(fig, \"plots/task_7/\"+training_set_name+\"many_convolutions_network_unnormalized.png\")\n",
    "\n",
    "\n",
    "    many_convolutions_model_batch_normalized = make_many_convolutions_model_batch_normalized(learning_x, learning_y)\n",
    "    \n",
    "    many_convolutions_history_batch_normalized = many_convolutions_model_batch_normalized.fit(learning_x, learning_y, epochs = my_n_epochs, batch_size=DEBUG_BATCH_SIZE, validation_split=0.2)\n",
    "    fig = plot_history(many_convolutions_history_batch_normalized, \"Task 7 \"+training_set_name+\" (Many convolutions (normalised)) \")\n",
    "    reckless_savefig(fig, \"plots/task_7/\"+training_set_name+\"many_convolutions_network_batch_normalized.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    task_7_1(\n",
    "        learning_x         = x_fashion_train_raw[:,:,:,None],\n",
    "        learning_y         = y_fashion_train,\n",
    "        training_set_name  =\"Fashion_MNIST\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
